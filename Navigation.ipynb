{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment by a deep-Q-network (DQN)\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict, namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "print(brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Here the DQN agent\n",
    "\n",
    "#### 4.1. Helper functions\n",
    "\n",
    "- `greedy_act` performs the greedy action selection. In this case the action-value function Q is the neural network (the model) defined in the next tab\n",
    "- `sample_random_from_buffer` takes a random sequence from the `buffer_memory` (where the sequences ($s_t,a_t,r_t,s_{t+1}$) at each time steps are stored)\n",
    "- `learn` optimizes the parameters of the neural network (the Q agent then) in order to match the target network Q' (the one built in the action greedy selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "def greedy_act(input_state, epsilon, model):\n",
    "    \"\"\"Returns actions for given state following an epsilon greedy policy.\n",
    "    \n",
    "    Params Input:\n",
    "    ======\n",
    "        state (np_array): current state\n",
    "        eps (float): epsilon for epsilon-greedy action selection\n",
    "        model (nn.Sequential): the DQN used to learn the Q function\n",
    "        \n",
    "    Return:\n",
    "    ======\n",
    "        action (integer in the range of action_size): the selected epsilon-greedy action\n",
    "    \"\"\"\n",
    "    state = torch.from_numpy(input_state).float().unsqueeze(0).to(device) #conversion of state in torch tensor\n",
    "        \n",
    "    #Evaluation time\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        action = model(state)\n",
    "        \n",
    "    #Training time\n",
    "    model.train()\n",
    "\n",
    "    # Epsilon-greedy action selection\n",
    "    if random.random() > epsilon:\n",
    "        return np.argmax(action.cpu().data.numpy())\n",
    "    else:\n",
    "        return random.choice(np.arange(action_size))\n",
    "        \n",
    "\n",
    "def sample_random_from_buffer(buffer_memory, batch_size):\n",
    "    \"\"\"Random samplig from the buffer memory\n",
    "    \n",
    "    Params Input:\n",
    "    ======\n",
    "        buffer_memory (deque(maxlen)): the buffer memory to store experiences \n",
    "        batch_size(int): the batch size (the # of expereinces pick up randomly form buffer_memory)\n",
    "        \n",
    "    Return:\n",
    "    ======\n",
    "        Tuple[torch.Tensor]: with states, actions, rewards, next states and dones\n",
    "    \"\"\"\n",
    "    experiences = random.sample(buffer_memory, k=batch_size)\n",
    "    states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "    actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "    dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "\n",
    "def learn(experiences, disc_fac, Q_target_model, Q_local_model, tau, lr):\n",
    "    \"\"\"Update value parameters with GD using given batch of experience tuples.\n",
    "\n",
    "        Params Input:\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            disc_fac (float): discount factor\n",
    "            Q_target_model (nn.Sequential): the DQN as target Q function\n",
    "            Q_local_model (nn.Sequential): the DQN as local Q function\n",
    "        \"\"\"\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Get max predicted Q values (for next states) from target model\n",
    "    Q_targets_next_action = Q_target_model(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    # Compute Q targets for current states \n",
    "    Q_targets = rewards + (disc_fac * Q_targets_next_action * (1 - dones))\n",
    "\n",
    "    # Get expected Q values from local model\n",
    "    Q_expected = Q_local_model(states).gather(1, actions)\n",
    "\n",
    "    ## Training\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(Q_expected, Q_targets)\n",
    "    # Minimize the loss\n",
    "    optimizer = optim.Adam(Q_local_model.parameters(), lr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Soft update (with prob tau) parameters of the Q target model  #\n",
    "    for target_params, local_params in zip(Q_target_model.parameters(), Q_local_model.parameters()):\n",
    "            target_params.data.copy_(tau*local_params.data + (1.0-tau)*target_params.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Inizialization\n",
    "\n",
    "After some inizilization (see the comment in the following code), in the next lines are defined the model for the action-value function Q and for the target action-value function Q '. We choose a neural net with two fully connected hidden layer of size 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inizialization\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "FIRST_HIDDEN = 64       # the size of the first hidden layer\n",
    "SECOND_HIDDEN = 64      # the size of the second hidden layer\n",
    "\n",
    "## The DQN-model\n",
    "\n",
    "# The neural net for the action value function Q\n",
    "dqn_model_local = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(state_size, FIRST_HIDDEN)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2',nn.Linear(FIRST_HIDDEN, SECOND_HIDDEN)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('output', nn.Linear(SECOND_HIDDEN,action_size))\n",
    "]))\n",
    "\n",
    "dqn_model_local.to(device)\n",
    "\n",
    "# The neural net for the target action value function Q'\n",
    "dqn_model_target = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(state_size, FIRST_HIDDEN)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2',nn.Linear(FIRST_HIDDEN, SECOND_HIDDEN)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('output', nn.Linear(SECOND_HIDDEN,action_size))\n",
    "]))\n",
    "\n",
    "dqn_model_target.to(device)\n",
    "\n",
    "\n",
    "## The replay buffer\n",
    "buffer_memory = deque(maxlen = BUFFER_SIZE)\n",
    "\n",
    "## The tuple for each experience\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. The code to run and to collect scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inizialization\n",
    "BATCH_SIZE = 64         # minibatch size of sequences in memory_buffer\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "def run_dqn_for_banana(n_episode=2000, max_t_per_episode=1000, eps_gre_in=1., eps_gre_end=0.01, eps_dec=0.995):\n",
    "    \"\"\"Deep-Q learning for banana Unity Environment.\n",
    "    \n",
    "    Params Input\n",
    "    ==========\n",
    "        n_episode (int): maximum number of episodes\n",
    "        max_t_per_episode (int): maximum time steps per episode (max of actions chosen per episode)\n",
    "        eps_gre_in (float): if epsilon greedy action selection, is the initial value of epsilon\n",
    "        eps_gre_end(float): if epsilon greedy action selection, is the ending value of epsilon\n",
    "        eps_dec (float): factor to decrease the epsilon value after each episode\n",
    "        \n",
    "    Params Output\n",
    "    ==========\n",
    "        scores (list of floats): are the scores collected at the end of each episode\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initalization\n",
    "    scores = []                       # list of scores for each episode\n",
    "    scores_window = deque(maxlen=100) # last 100 scores (to plot the avarage in a window of the last 100 scores)\n",
    "    epsilon = eps_gre_in              # starting epsilon\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episode+1):\n",
    "        init_env_info = env.reset(train_mode=True)[brain_name]   # initial env infos\n",
    "        state = init_env_info.vector_observations[0]             # initial state after reset env\n",
    "        score = 0.0                                              # initial score\n",
    "        \n",
    "        for t in range(max_t_per_episode):\n",
    "            action = greedy_act(state, epsilon, dqn_model_local)      #epsilon-greedy action selection\n",
    "            env_info_after_step = env.step(action)[brain_name]        #performs the selected action on the env\n",
    "            next_state = env_info_after_step.vector_observations[0]   #get the next state\n",
    "            reward = env_info_after_step.rewards[0]                   #get the reward\n",
    "            done = env_info_after_step.local_done[0]                  #done status\n",
    "            \n",
    "            #store the sequence (s,a,r',s') on the buffer memory\n",
    "            exp = experience(state, action, reward, next_state, done) \n",
    "            buffer_memory.append(exp)\n",
    "            \n",
    "            #perform the training of the Q-neural-net each UPDATE_EVERY\n",
    "            tmp_t = (t+1) % UPDATE_EVERY\n",
    "            if  tmp_t == 0:\n",
    "                if len(buffer_memory) > BATCH_SIZE:\n",
    "                    experiences = sample_random_from_buffer(buffer_memory, BATCH_SIZE) #sample BATCH_SIZE sequences from the buffer_memory\n",
    "                    learn(experiences, GAMMA, dqn_model_target, dqn_model_local, TAU, LR) #train the Q-neural-net\n",
    "             \n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        epsilon = max(eps_gre_end, eps_dec*epsilon) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(dqn_model_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Print and plot of the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = run_dqn_for_banana()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
